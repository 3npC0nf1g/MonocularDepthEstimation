{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 1: Import libraries and data processing**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f42ee6dc2d9e86b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning -U torchinfo segmentation_models_pytorch open3d albumentations opencv-python-headless scipy scikit-image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "442c5f7d75e1518f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image\n",
    "from skimage import io\n",
    "import torchvision\n",
    "import torchvision.models.resnet as resnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchinfo import summary\n",
    "import open3d as o3d\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f94a7e1900831fd5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 2: Data Loading and Preprocessing**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281a57baba74c3ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Set paths and parameters\n",
    "dataset_path = \"./dataset/\"  # Update this path to your dataset location\n",
    "image_size = (256, 256)  # Resize dimensions\n",
    "\n",
    "# Step 2: Load dataset\n",
    "images = []\n",
    "depths = []\n",
    "\n",
    "# Assuming dataset structure: ./dataset/images/ and ./dataset/depth/\n",
    "image_dir = os.path.join(dataset_path, 'images')\n",
    "depth_dir = os.path.join(dataset_path, 'depth')\n",
    "\n",
    "image_files = sorted(os.listdir(image_dir))\n",
    "depth_files = sorted(os.listdir(depth_dir))\n",
    "\n",
    "for img_file, depth_file in zip(image_files, depth_files):\n",
    "    # Read RGB image\n",
    "    img = cv2.imread(os.path.join(image_dir, img_file))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, image_size)\n",
    "    images.append(img)\n",
    "\n",
    "    # Read depth map\n",
    "    depth = cv2.imread(os.path.join(depth_dir, depth_file), cv2.IMREAD_UNCHANGED)\n",
    "    depth = cv2.resize(depth, image_size)\n",
    "    depths.append(depth)\n",
    "\n",
    "images = np.array(images, dtype=np.float32) / 255.0  # Normalize images\n",
    "depths = np.array(depths, dtype=np.float32)\n",
    "\n",
    "print(f\"Images Shape: {images.shape}, Depths Shape: {depths.shape}\")\n",
    "\n",
    "# Step 3: Split data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, depths, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training Set Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation Set Shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing Set Shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Step 4: Data Augmentation (optional)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Example: Augment training images\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "data_gen.fit(X_train)\n",
    "\n",
    "# Display a few augmented images\n",
    "for X_batch, y_batch in data_gen.flow(X_train, y_train, batch_size=5):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow(X_batch[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "# Step 5: Save preprocessed data\n",
    "np.savez_compressed('preprocessed_data.npz', \n",
    "                     X_train=X_train, y_train=y_train, \n",
    "                     X_val=X_val, y_val=y_val, \n",
    "                     X_test=X_test, y_test=y_test)\n",
    "\n",
    "print(\"Preprocessed dataset saved successfully!\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e721ff9c1e39932"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
